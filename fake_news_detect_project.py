# -*- coding: utf-8 -*-
"""fake news detect project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qcNf6P9kTRIQ3y8EMgiG50JylaO6V1ub
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install transformers tqdm

# ✅ Optimized BERT Fake News Detection (Colab RAM Friendly Version)
import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

# Load datasets
real = pd.read_csv("/content/drive/MyDrive/FN True.csv")
fake = pd.read_csv("/content/drive/MyDrive/FN Fake.csv")
real["label"] = 1
fake["label"] = 0
df = pd.concat([real, fake], axis=0).reset_index(drop=True)

# ✅ Use only 'title' to reduce memory usage
df["content"] = df["title"]
df = df[["content", "label"]]

# ✅ Use only a subset for memory efficiency (optional)
df = df.sample(frac=1).reset_index(drop=True)  # Shuffle
df = df[:3000]  # Use only first 3000 records

X_train, X_test, y_train, y_test = train_test_split(df["content"], df["label"], test_size=0.2, random_state=42)

# Dataset class
class NewsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=256):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        encoding = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# ✅ Use smaller batch size to avoid OOM
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_dataset = NewsDataset(X_train.tolist(), y_train.tolist(), tokenizer)
test_dataset = NewsDataset(X_test.tolist(), y_test.tolist(), tokenizer)
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

# Train
model.train()
for batch in tqdm(train_loader):
    input_ids = batch["input_ids"].to(device)
    attention_mask = batch["attention_mask"].to(device)
    labels = batch["labels"].to(device)

    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    optimizer.zero_grad()

# Save model
torch.save(model.state_dict(), "bert_fake_news_model.pt")

# Evaluate
model.eval()
predictions, true_labels = [], []
with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()
        predictions.extend(preds)
        true_labels.extend(batch["labels"].numpy())

print("Accuracy:", accuracy_score(true_labels, predictions))
print("Classification Report:")
print(classification_report(true_labels, predictions))